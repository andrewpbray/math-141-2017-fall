---
title: "Bayesian Modeling"
output:
  ioslides_presentation:
    incremental: true
---

```{r setup, include=FALSE}
library(knitr)
options(digits=3)
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", warning = FALSE, message = FALSE)
library(dplyr)
library(ggplot2)
```


##

Please draw your own subjective distributions for the following events.

1. The probability that it will snow at Reed this winter.
2. The probability that, on a given night, the sun has gone super nova.
3. The total number of individual socks that you own.


## Karl Broman's Socks {.flexbox .vcenter .build}
```{r echo = FALSE, eval = FALSE}
# credit for this example goes to Rasmus Baathe, who made a similar presentation
# at userR! 2015. most of the included images are his.
```

<img src="../figs/broman-tweet.png" width="400px" />


## Classical H test {.build}

### Assert a model
$H_0$: I have $N_{pairs}$ pairs of socks and $N_{singles}$ singletons. The first 11 socks that 
I pull out of the machine are a random sample from this population.

### Decide on a test statistic
The number of singletons in the sample: 11.

### Construct the sampling distribution
Probability theory or simulation.

### See where your observed stat lies in that distribution
Find the p-value if you like.


## $H_0$ {.flexbox .vcenter .build}

<img src="../figs/pairs-socks.png" height="400px" />

$$N_{pairs} = 9$$


## $H_0$ {.flexbox .vcenter .build}

<img src="../figs/all-socks.png" height="400px" />

$$N_{pairs} = 9; \quad N_{singles} = 5$$


## Contructing the sampling dist. {.build}

We'll use simulation.

Create the population of socks:

```{r}
sock_pairs <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K")
sock_singles <- c("l", "m", "n", "o", "p")
socks <- c(rep(sock_pairs, each = 2), sock_singles)
socks
```


## One draw from the machine {.build}

```{r}
picked_socks <- sample(socks, size = 11, replace = FALSE)
picked_socks
sock_counts <- table(picked_socks)
sock_counts
n_singles <- sum(sock_counts == 1)
n_singles
```


## Our simulator {.flexbox .vcenter .build}

<img src="../figs/washing-machine.png" height="400px" />


## Constructing the sampling dist. {.build}

```{r echo = FALSE}
pick_socks <- function(N_pairs, N_singles, N_pick) {
  N_sock_types <- N_pairs + N_singles
  socks <- rep(1:N_sock_types, rep( 2:1, c(N_pairs, N_singles) ))
  picked_socks <- sample(socks, 11)
  sock_counts <- table(picked_socks)
  n_singles <- sum(sock_counts == 1)
  n_singles
}
set.seed(200)
```

```{r}
pick_socks(N_pairs = 9, N_singles = 5, N_pick = 11)
pick_socks(9, 5, 11)
pick_socks(9, 5, 11)
```

Repeat many, many times...


## The sampling distribution {.flexbox .vcenter}

```{r echo = FALSE, cache = TRUE}
sim_singles <- rep(0, 1000)

for (i in 1:1000) {
  sim_singles[i] <- pick_socks(9, 5, 11)
}

qplot(as.factor(sim_singles), geom = "bar", xlab = "number of singletons")
```


## The sampling distribution {.flexbox .vcenter}

```{r echo = FALSE}
qplot(as.factor(sim_singles), geom = "bar", xlab = "number of singletons") +
  geom_vline(xintercept = 6, col = "tomato")
```


## The p-value {.build}

Quantifying how far into the tails our observed count was.

```{r}
table(sim_singles)
table(sim_singles)[6]/1000
```

```{r echo = FALSE}
pval <- table(sim_singles)[6]/1000
```

Our two-tailed p-value is `r pval*2`.


## Question

What is the best definition for our p-value in probability notation?

1. P($H_0$ is true | data) = `r pval`
2. P($H_0$ is false | data) = `r pval`
3. P($H_0$ is true) = `r pval`
4. P(data | $H_0$ is true) = `r pval`
5. P(data) = `r pval`


## Question

What is the best definition for our p-value in probability notation?

1. P($H_0$ is true | data) = `r pval`
2. P($H_0$ is false | data) = `r pval`
3. P($H_0$ is true) = `r pval`
4. **P(data | $H_0$ is true) = `r pval`**
5. P(data) = `r pval`


## The challenges with the classical method {.build}

The result of a hypothesis test is a probability of the form:

$$ P(\textrm{ data or more extreme } | \ H_0 \textrm{ true }) $$

while most people *think* they're getting

$$ P(\ H_0 \textrm{ true } | \textrm{ data }) $$

How can we go from the former to the latter?


## What we have {.flexbox .vcenter}
<img src="../figs/classical-socks.png" width="800px" />


## What we want {.flexbox .vcenter}
<img src="../figs/bayes-socks.png" width="800px" />


# Bayesian Modeling
## Bayes Rule {.build}

$$P(A \ | \ B) = \frac{P(A \textrm{ and } B)}{P(B)} $$

$$P(A \ | \ B) = \frac{P(B \ | \ A) \ P(A)}{P(B)} $$

$$P(model \ | \ data) = \frac{P(data \ | \ model) \ P(model)}{P(data)} $$

What does it mean to think about $P(model)$?


##

Please draw your own subjective distributions for the following events.

1. The probability that it will snow at Reed this winter.
2. The probability that, on a given night, the sun has gone super nova.
3. The total number of individual socks that you own.


## Prior distribution {.build .flexbox .vcenter}

A *prior distribution* is a probability distribution for a *parameter* that 
summarizes the information that you have before seeing the data.

```{r, cache = TRUE, echo = FALSE}
x <- rnbinom(1e6, mu = 30, size = -30^2 / (30 - 15^2))
(prior_n <- qplot(x, geom = "histogram", xlab = "number of socks", binwidth = 1, xlim = c(0, 100),
      fill = I("green"), ylab = "prob. density", main = "P(parameter)"))
```


## Prior on proportion pairs {.flexbox .vcenter .build}

```{r cache = TRUE, echo = FALSE}
y <- rbeta(1e6, shape1 = 15, shape2 = 2)
(prior_p <- qplot(y, geom = "histogram", xlab = "proportion of pairs", binwidth = .01, xlim = c(0, 1),
      fill = I("green"), ylab = "prob. density", main = "P(parameter)"))
```


## {.flexbox .vcenter}
<img src="../figs/abc1.png" height="550px" />


## {.flexbox .vcenter}
<img src="../figs/abc2.png" height="550px" />


## {.flexbox .vcenter}
<img src="../figs/abc3.png" height="550px" />


## {.flexbox .vcenter}
<img src="../figs/abc4.png" height="550px" />


## {.flexbox .vcenter}
<img src="../figs/abc5.png" height="550px" />


## {.flexbox .vcenter}
<img src="../figs/abc6.png" height="550px" />


## {.flexbox .vcenter}
<img src="../figs/abc7.png" height="550px" />


## Full simulation {.build}

```{r cache = TRUE, echo = FALSE}
sock_sim <- t(replicate(100000, {
  n_socks <- rnbinom(1, mu = 30, size = -30^2 / (30 - 15^2) )
  prop_pairs <- rbeta(1, shape1 = 15, shape2 = 2)
  n_pairs <- round(floor(n_socks / 2) * prop_pairs)
  n_odd <- n_socks - n_pairs * 2
  n_sock_types <- n_pairs + n_odd
  socks <- rep(seq_len(n_sock_types), rep( 2:1, c(n_pairs, n_odd) ))
  picked_socks <- sample(socks, size =  min(11, n_socks))
  sock_counts <- table(picked_socks)
  c(unique = sum(sock_counts == 1), pairs = sum(sock_counts == 2),
    n_socks = n_socks, prop_pairs = prop_pairs)
}))
sock_sim <- as.data.frame(sock_sim)
post_samples <- sock_sim %>%
  filter(unique == 11, pairs == 0)
```

```{r}
head(sock_sim)
sock_sim %>%
  filter(unique == 11, pairs == 0) %>%
  head()
```


## Proportion of pairs

```{r echo = FALSE}
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

```{r cache = TRUE, echo = FALSE, fig.width = 8.5}
post_p <- qplot(prop_pairs, data = post_samples, geom = "histogram", 
                xlab = "proportion of pairs", binwidth = .01, xlim = c(0, 1),
                fill = I("green"), ylab = "prob. density", main = "P(parameter|data)")
multiplot(prior_p, post_p, layout = matrix(1:2, ncol = 2, byrow = TRUE))
```


## Number of socks

```{r cache = TRUE, echo = FALSE, fig.width = 8.5}
post_n <- qplot(n_socks, data = post_samples, geom = "histogram", 
                xlab = "number of socks", binwidth = 1, xlim = c(0, 100),
                fill = I("green"), ylab = "prob. density", main = "P(parameter|data)")
multiplot(prior_n, post_n, layout = matrix(1:2, ncol = 2, byrow = TRUE))
```


## Karl Broman's Socks {.flexbox .vcenter .build}

<img src="../figs/broman-tweet.png" width="400px" />


## The posterior distribution {.build}

```{r cache = TRUE, echo = FALSE, fig.height = 3, fig.width = 5}
post_n
```

- Distribution of a parameter after conditioning on the data
- Synthesis of prior knowledge and observations (data)

### Question
What is your best guess for the number of socks that Karl has?


## Our best guess

```{r cache = TRUE, echo = FALSE, fig.height = 3, fig.width = 5}
qplot(n_socks, data = post_samples, geom = "histogram", 
                xlab = "number of socks", binwidth = 1, xlim = c(0, 100),
                fill = I("green"), ylab = "prob. density", main = "P(parameter|data)") +
  geom_vline(xintercept = median(post_samples$n_socks), col = "goldenrod")
```

- The posterior median is 44 socks.


## Karl Broman's Socks {.flexbox .vcenter .build}

<img src="../figs/broman-tweet2.png" width="600px" />

$$ 21 \times 2 + 3 = 45 \textrm{ socks} $$


## Summary {.build}

Bayesian methods . . .

- Require the subjective specification of your prior knowledge
- Provide a posterior distribution on the parameters
- Have strong intuition
- Are computationally expensive


##  {.flexbox .vcenter .build}

<img src="../figs/supernova.png" height="550px" />

